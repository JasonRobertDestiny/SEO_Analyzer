#!/usr/bin/env python3
"""
è½»é‡çº§AIåˆ†ææ¨¡å—
æä¾›å¿«é€Ÿã€ç®€åŒ–çš„AI SEOåˆ†æåŠŸèƒ½
"""
import asyncio
import time
import logging
from typing import Dict, List, Optional, Any, Union
import json

logger = logging.getLogger(__name__)


class LightweightAIAnalyzer:
    """è½»é‡çº§AIåˆ†æå™¨"""
    
    def __init__(self, enable_cache: bool = True):
        self.enable_cache = enable_cache
        self.cache = {} if enable_cache else None
        self.analysis_patterns = self._load_analysis_patterns()
    
    def _load_analysis_patterns(self) -> Dict[str, Any]:
        """åŠ è½½åˆ†ææ¨¡å¼"""
        return {
            "title_patterns": {
                "good": ["åŒ…å«å…³é”®è¯", "é•¿åº¦é€‚ä¸­", "å…·æœ‰å¸å¼•åŠ›"],
                "bad": ["è¿‡é•¿", "è¿‡çŸ­", "é‡å¤", "æ— å…³é”®è¯"]
            },
            "meta_patterns": {
                "good": ["150-160å­—ç¬¦", "åŒ…å«å…³é”®è¯", "è¡ŒåŠ¨å‘¼å"],
                "bad": ["è¿‡é•¿", "è¿‡çŸ­", "é‡å¤", "æ— æè¿°"]
            },
            "content_patterns": {
                "good": ["ç»“æ„æ¸…æ™°", "å…³é”®è¯å¯†åº¦é€‚ä¸­", "åŸåˆ›æ€§é«˜"],
                "bad": ["å†…å®¹ç¨€å°‘", "å…³é”®è¯å †ç Œ", "é‡å¤å†…å®¹"]
            }
        }
    
    async def quick_ai_enhance(self, seo_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¿«é€ŸAIå¢å¼ºåˆ†æ"""
        try:
            start_time = time.time()
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(seo_data)
            if self.enable_cache and cache_key in self.cache:
                logger.info("ä½¿ç”¨ç¼“å­˜çš„AIåˆ†æç»“æœ")
                return self.cache[cache_key]
            
            # å¿«é€Ÿæ¨¡å¼åˆ†æ
            enhancement = await self._analyze_fast_mode(seo_data)
            
            # æ·»åŠ åˆ†ææ—¶é—´
            enhancement["ai_analysis_time"] = time.time() - start_time
            enhancement["analysis_mode"] = "lightweight"
            
            # ç¼“å­˜ç»“æœ
            if self.enable_cache:
                self.cache[cache_key] = enhancement
            
            logger.info(f"è½»é‡çº§AIåˆ†æå®Œæˆ ({enhancement['ai_analysis_time']:.2f}ç§’)")
            return enhancement
            
        except Exception as e:
            logger.error(f"è½»é‡çº§AIåˆ†æå¤±è´¥: {e}")
            return self._create_fallback_result(seo_data)
    
    async def _analyze_fast_mode(self, seo_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¿«é€Ÿæ¨¡å¼åˆ†æ"""
        pages = seo_data.get("pages", [])
        if not pages:
            return self._create_empty_result()
        
        # åˆ†æä¸»é¡µé¢
        main_page = pages[0]
        
        # å¿«é€Ÿåˆ†æå„ä¸ªæ–¹é¢
        title_analysis = self._analyze_title_fast(main_page.get("title", ""))
        meta_analysis = self._analyze_meta_fast(main_page.get("meta_description", ""))
        content_analysis = self._analyze_content_fast(main_page)
        
        # ç”Ÿæˆç®€åŒ–å»ºè®®
        recommendations = self._generate_quick_recommendations(
            title_analysis, meta_analysis, content_analysis
        )
        
        return {
            "ai_insights": {
                "title_score": title_analysis["score"],
                "meta_score": meta_analysis["score"],
                "content_score": content_analysis["score"],
                "overall_score": (title_analysis["score"] + meta_analysis["score"] + content_analysis["score"]) / 3
            },
            "quick_recommendations": recommendations,
            "analysis_summary": self._create_summary(title_analysis, meta_analysis, content_analysis),
            "optimization_priority": self._determine_priority(title_analysis, meta_analysis, content_analysis)
        }
    
    def _analyze_title_fast(self, title: str) -> Dict[str, Any]:
        """å¿«é€Ÿæ ‡é¢˜åˆ†æ"""
        if not title:
            return {"score": 0, "issues": ["ç¼ºå°‘æ ‡é¢˜"], "suggestions": ["æ·»åŠ é¡µé¢æ ‡é¢˜"]}
        
        score = 50  # åŸºç¡€åˆ†æ•°
        issues = []
        suggestions = []
        
        # é•¿åº¦æ£€æŸ¥
        length = len(title)
        if length < 30:
            issues.append("æ ‡é¢˜è¿‡çŸ­")
            suggestions.append("æ‰©å±•æ ‡é¢˜é•¿åº¦è‡³30-60å­—ç¬¦")
        elif length > 60:
            issues.append("æ ‡é¢˜è¿‡é•¿")
            suggestions.append("ç¼©çŸ­æ ‡é¢˜è‡³60å­—ç¬¦ä»¥å†…")
            score -= 20
        else:
            score += 30
        
        # å…³é”®è¯æ£€æŸ¥ (ç®€åŒ–ç‰ˆ)
        if any(word in title.lower() for word in ["seo", "ä¼˜åŒ–", "ç½‘ç«™", "åˆ†æ"]):
            score += 20
        else:
            suggestions.append("åœ¨æ ‡é¢˜ä¸­åŒ…å«ç›¸å…³å…³é”®è¯")
        
        return {
            "score": min(100, max(0, score)),
            "issues": issues,
            "suggestions": suggestions
        }
    
    def _analyze_meta_fast(self, meta_description: str) -> Dict[str, Any]:
        """å¿«é€ŸMetaæè¿°åˆ†æ"""
        if not meta_description:
            return {
                "score": 0,
                "issues": ["ç¼ºå°‘Metaæè¿°"],
                "suggestions": ["æ·»åŠ 150-160å­—ç¬¦çš„Metaæè¿°"]
            }
        
        score = 50
        issues = []
        suggestions = []
        
        length = len(meta_description)
        if length < 120:
            issues.append("Metaæè¿°è¿‡çŸ­")
            suggestions.append("æ‰©å±•Metaæè¿°è‡³120-160å­—ç¬¦")
        elif length > 160:
            issues.append("Metaæè¿°è¿‡é•¿")
            suggestions.append("ç¼©çŸ­Metaæè¿°è‡³160å­—ç¬¦ä»¥å†…")
            score -= 15
        else:
            score += 40
        
        return {
            "score": min(100, max(0, score)),
            "issues": issues,
            "suggestions": suggestions
        }
    
    def _analyze_content_fast(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¿«é€Ÿå†…å®¹åˆ†æ"""
        score = 50
        issues = []
        suggestions = []
        
        # æ£€æŸ¥H1æ ‡ç­¾
        h1_tags = page_data.get("h1", [])
        if not h1_tags:
            issues.append("ç¼ºå°‘H1æ ‡ç­¾")
            suggestions.append("æ·»åŠ ä¸»è¦H1æ ‡é¢˜")
            score -= 20
        elif len(h1_tags) > 1:
            issues.append("å¤šä¸ªH1æ ‡ç­¾")
            suggestions.append("æ¯é¡µåªä½¿ç”¨ä¸€ä¸ªH1æ ‡ç­¾")
            score -= 10
        else:
            score += 20
        
        # æ£€æŸ¥å­—æ•°
        word_count = page_data.get("word_count", 0)
        if word_count < 300:
            issues.append("å†…å®¹è¿‡å°‘")
            suggestions.append("å¢åŠ é¡µé¢å†…å®¹è‡³300å­—ä»¥ä¸Š")
            score -= 15
        else:
            score += 20
        
        return {
            "score": min(100, max(0, score)),
            "issues": issues,
            "suggestions": suggestions
        }
    
    def _generate_quick_recommendations(self, title_analysis: Dict, meta_analysis: Dict, content_analysis: Dict) -> List[str]:
        """ç”Ÿæˆå¿«é€Ÿå»ºè®®"""
        recommendations = []
        
        # æ ¹æ®å…·ä½“é—®é¢˜ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
        if title_analysis["score"] < 70:
            recommendations.extend([
                f"ğŸ¯ ä¼˜åŒ–é¡µé¢æ ‡é¢˜ï¼š{', '.join(title_analysis['suggestions'][:2])}",
                "ğŸ“ ç¡®ä¿æ ‡é¢˜åŒ…å«ä¸»è¦å…³é”®è¯å¹¶æ§åˆ¶åœ¨30-60å­—ç¬¦",
                "âœ¨ è®©æ ‡é¢˜æ›´æœ‰å¸å¼•åŠ›ï¼Œæé«˜ç‚¹å‡»ç‡"
            ])
        
        if meta_analysis["score"] < 70:
            recommendations.extend([
                f"ğŸ“„ æ”¹è¿›Metaæè¿°ï¼š{', '.join(meta_analysis['suggestions'][:2])}",
                "ğŸ¯ åœ¨æè¿°ä¸­åŒ…å«è¡ŒåŠ¨å·å¬è¯­è¨€",
                "ğŸ“ ä¿æŒMetaæè¿°åœ¨150-160å­—ç¬¦ä¹‹é—´"
            ])
        
        if content_analysis["score"] < 70:
            recommendations.extend([
                f"ğŸ“‹ ä¼˜åŒ–é¡µé¢å†…å®¹ï¼š{', '.join(content_analysis['suggestions'][:2])}",
                "ğŸ”— æ·»åŠ æ›´å¤šå†…éƒ¨é“¾æ¥æå‡é¡µé¢æƒé‡",
                "ğŸ“Š å¢åŠ é¡µé¢å†…å®¹æ·±åº¦å’Œç›¸å…³æ€§"
            ])
        
        # æ·»åŠ é€šç”¨ä¼˜åŒ–å»ºè®®
        if len(recommendations) < 5:
            recommendations.extend([
                "ğŸš€ æå‡é¡µé¢åŠ è½½é€Ÿåº¦ï¼Œä¼˜åŒ–ç”¨æˆ·ä½“éªŒ",
                "ğŸ“± ç¡®ä¿ç½‘ç«™å“åº”å¼è®¾è®¡é€‚é…ç§»åŠ¨è®¾å¤‡",
                "ğŸ” ç ”ç©¶ç«äº‰å¯¹æ‰‹å…³é”®è¯ç­–ç•¥",
                "ğŸ“ˆ è®¾ç½®Google Analyticsè·Ÿè¸ªåˆ†ææ•ˆæœ",
                "ğŸ’¡ å®šæœŸæ›´æ–°å†…å®¹ä¿æŒç½‘ç«™æ´»è·ƒåº¦"
            ])
        
        return recommendations[:8]  # è¿”å›æœ€å¤š8ä¸ªå»ºè®®
    
    def _create_summary(self, title_analysis: Dict, meta_analysis: Dict, content_analysis: Dict) -> str:
        """åˆ›å»ºåˆ†ææ‘˜è¦"""
        overall_score = (title_analysis["score"] + meta_analysis["score"] + content_analysis["score"]) / 3
        
        if overall_score >= 80:
            return "é¡µé¢SEOä¼˜åŒ–è‰¯å¥½ï¼Œåªéœ€è¦å¾®è°ƒå³å¯ã€‚"
        elif overall_score >= 60:
            return "é¡µé¢SEOæœ‰æ”¹è¿›ç©ºé—´ï¼Œå»ºè®®ä¼˜åŒ–æ ‡é¢˜å’Œå†…å®¹ã€‚"
        elif overall_score >= 40:
            return "é¡µé¢SEOéœ€è¦é‡è¦æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯å…ƒç´ æ ‡ç­¾å’Œå†…å®¹ç»“æ„ã€‚"
        else:
            return "é¡µé¢SEOäºŸéœ€å…¨é¢ä¼˜åŒ–ï¼Œå»ºè®®ä»åŸºç¡€å…ƒç´ å¼€å§‹æ”¹è¿›ã€‚"
    
    def _determine_priority(self, title_analysis: Dict, meta_analysis: Dict, content_analysis: Dict) -> List[str]:
        """ç¡®å®šä¼˜åŒ–ä¼˜å…ˆçº§"""
        priorities = []
        
        scores = [
            ("æ ‡é¢˜ä¼˜åŒ–", title_analysis["score"]),
            ("Metaæè¿°ä¼˜åŒ–", meta_analysis["score"]),
            ("å†…å®¹ä¼˜åŒ–", content_analysis["score"])
        ]
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œåˆ†æ•°ä½çš„ä¼˜å…ˆçº§é«˜
        scores.sort(key=lambda x: x[1])
        
        return [item[0] for item in scores]
    
    def _generate_cache_key(self, seo_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨é¡µé¢URLå’Œä¸»è¦å†…å®¹ç”Ÿæˆç®€å•çš„hash
        pages = seo_data.get("pages", [])
        if pages:
            first_page = pages[0]
            key_data = f"{first_page.get('url', '')}-{first_page.get('title', '')}"
            return str(hash(key_data))
        return "default"
    
    def _create_fallback_result(self, seo_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå›é€€ç»“æœ"""
        return {
            "ai_insights": {
                "title_score": 50,
                "meta_score": 50,
                "content_score": 50,
                "overall_score": 50
            },
            "quick_recommendations": [
                "æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯",
                "ç¡®ä¿Metaæè¿°é•¿åº¦åœ¨150-160å­—ç¬¦",
                "æ·»åŠ é€‚å½“çš„H1æ ‡ç­¾",
                "å¢åŠ é¡µé¢å†…å®¹ä¸°å¯Œåº¦"
            ],
            "analysis_summary": "ä½¿ç”¨åŸºç¡€åˆ†ææ¨¡å¼ï¼Œå»ºè®®è¿›è¡Œæ ‡å‡†SEOä¼˜åŒ–ã€‚",
            "optimization_priority": ["æ ‡é¢˜ä¼˜åŒ–", "Metaæè¿°ä¼˜åŒ–", "å†…å®¹ä¼˜åŒ–"],
            "ai_analysis_time": 0.1,
            "analysis_mode": "fallback"
        }
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            "ai_insights": {
                "title_score": 0,
                "meta_score": 0,
                "content_score": 0,
                "overall_score": 0
            },
            "quick_recommendations": ["æ— æ³•åˆ†æé¡µé¢ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"],
            "analysis_summary": "æœªèƒ½è·å–é¡µé¢æ•°æ®è¿›è¡Œåˆ†æã€‚",
            "optimization_priority": [],
            "ai_analysis_time": 0.01,
            "analysis_mode": "empty"
        }


# å…¨å±€å®ä¾‹
_global_analyzer = None

async def quick_ai_enhance(seo_data: Dict[str, Any], use_cache: bool = True) -> Dict[str, Any]:
    """å¿«é€ŸAIå¢å¼ºåˆ†æå…¥å£å‡½æ•°"""
    global _global_analyzer
    
    if _global_analyzer is None:
        _global_analyzer = LightweightAIAnalyzer(enable_cache=use_cache)
    
    return await _global_analyzer.quick_ai_enhance(seo_data)


def create_lightweight_analyzer(enable_cache: bool = True) -> LightweightAIAnalyzer:
    """åˆ›å»ºè½»é‡çº§åˆ†æå™¨å®ä¾‹"""
    return LightweightAIAnalyzer(enable_cache=enable_cache)


async def batch_quick_enhance(seo_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """æ‰¹é‡å¿«é€Ÿå¢å¼º"""
    analyzer = LightweightAIAnalyzer()
    tasks = [analyzer.quick_ai_enhance(data) for data in seo_data_list]
    return await asyncio.gather(*tasks, return_exceptions=True)


# é¢„è®¾åˆ†æé…ç½®
class LightweightConfig:
    """è½»é‡çº§é…ç½®"""
    
    ULTRA_FAST = {
        "enable_cache": True,
        "analysis_depth": "basic",
        "timeout": 5
    }
    
    BALANCED = {
        "enable_cache": True,
        "analysis_depth": "standard",
        "timeout": 10
    }
    
    THOROUGH = {
        "enable_cache": False,
        "analysis_depth": "detailed",
        "timeout": 20
    }
